---
title: "Lecture 30:  Review"
shorttitle: "STATS 191 Lecture 30"
author: "Pratheepa Jeganathan"
date: "12/06/2019"
output: 
  beamer_presentation:
    colortheme: "seahorse"
    slide_level: 2
    includes:
      in_header: header.tex
bibliography: AppliedStat.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 4, message=FALSE, warning=FALSE, cache = TRUE)
set.seed(0)
library(ggplot2)
library(magrittr)
library(dplyr)
```

## Course Evaluations Now Open

- Axess is now open to complete end-term course evaluations.
- You can find it on
    - Axess > Student > Course and Section Evaluations
- If you complete all of the feedback you will see your grades by 12/13/2019 otherwise 12/17/2019

## Final examination
- Final examination information
  - In-class examination.
  - Time: According the [\blc Stanford calendar\bc](https://registrar.stanford.edu/autumn-quarter-exams): \rc Wednesday, December 11, 2019 @ 3:30PM-6:30 PM\bc.
  - Location: [\blc Skilling Auditorium\bc](https://campus-map.stanford.edu/#).
  
```{r echo=FALSE,out.width = "250px"}
knitr::include_graphics("location_building.png")
```

## Final examination

- Students **are not allowed** to take final examinations earlier than the scheduled date and time (except for the event of extraordinary circumstance that is determined solely by me).

- What to bring
  - \uppercase{A calculator.}
  - \uppercase{Four single-sided pages of notes.}
  


## Expected outcomes

By the end of the course, students should be able to: 

- Enter tabular data using [R](http://cran.r-project.org).
- Plot data using [R](http://cran.r-project.org), to help in exploratory data analysis.
- Formulate regression models for the data, while understanding some of the limitations and assumptions implicit in using these models.
- Fit models using [R](http://cran.r-project.org) and interpret the output.
- Test for associations in a given model.

## Expected outcomes (cont.)
- Use diagnostic plots and tests to assess the adequacy of a particular model.
- Find confidence intervals for the effects of different explanatory variables in the model.
- Use some basic model selection procedures, as found in [R](http://cran.r-project.org), to find a *best* model in a class of models.
- Fit simple ANOVA models in [R](http://cran.r-project.org), treating them as special cases of multiple regression models.
- Fit simple logistic and \rc Poisson \bc regression models.

##  Evaluation

The final letter grade for this course will be determined by each method of assessment weighted as follows:

- 7 weekly homework assignments  (55%)
- Midterm examination (15%)
- Final examination (30%)
- Quiz and Bonus points (5%+5.2%) 

The final percentage to letter grade conversion:

\begin{tabular}{lll}
A+ = 97-110.2 & A = 96-94 & A- = 90-93\\
B+ = 87-89 & B = 84-86 & B- = 80-83 \\
C+ = 77-79 & C = 74-76 & C- = 70-73 \\
D+ = 67-69 & D = 64-66 & D- = 60-63 
\end{tabular}


## Topics covered

- Simple linear regression.
- Diagnostics for simple linear regression.
- Multiple linear regression.
- Diagnostics.
- Interactions and ANOVA.
- Weighted Least Squares.
- Autocorrelation.
- Bootstrapping `lm`.
- Model selection.
- Multicollinearity.
- Penalized regression.
- Logistic regression.

# Simple linear regression


## Least squares

- We used "least squares" regression. This measures the goodness of fit of a line by the sum of squared errors, $SSE$.
- Least squares regression chooses the line that minimizes $SSE(\beta_0, \beta_1) = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 \cdot X_i)^2.$

## Geometry of least squares
- The following picture depicts the geometry involved in least squares regression.

```{r echo=FALSE,out.width = "200px"}
knitr::include_graphics("axes_simple.png")
```


## What is a $t$-statistic?

- Start with $Z \sim N(0,1)$ is standard normal and $S^2 \sim \chi^2_{\nu}$, independent of $Z$.
- Compute $T = \frac{Z}{\sqrt{\frac{S^2}{\nu}}}.$
- Then, $T \sim t_{\nu}$ has a $t$-distribution with $\nu$ degrees of freedom.
- Generally, a $t$-statistic has the form $$ T = \frac{\hat{\theta} - \theta}{SE(\hat{\theta})}$$

## Interval Estimates

- A $(1-\alpha) \cdot 100 \%$ confidence interval: $$\widehat{\beta}_1 \pm SE(\widehat{\beta}_1) \cdot t_{n-2, 1-\alpha/2}.$$

- Interval for regression line $\beta_0 + \beta_1 \cdot X$
    - $(1-\alpha) \cdot 100 \%$ confidence interval for $\beta_0 + \beta_1 X$: $$\widehat{\beta}_0 + \widehat{\beta}_1 X \pm SE(\widehat{\beta}_0 + \widehat{\beta}_1 X) \cdot t_{n-2, 1-\alpha/2}$$ where $SE(\widehat{\beta}_0 + \widehat{\beta}_1) = \widehat{\sigma} \sqrt{\frac{1}{n} + \frac{(\overline{X} - X)^2}{\sum_{i=1}^n \left(X_i-\overline{X}\right)^2}}$


## Interval Estimates 

- Prediction intervals for $\beta_0 +\beta_1 X_{new} + \epsilon_{new}$
    - $(1-\alpha) \cdot 100 \%$ prediction interval for $\beta_0 +\beta_1 X_{new} + \epsilon_{new}$ is 
    $$\widehat{\beta}_0 +  \widehat{\beta}_1 X_{\text{new}} \pm t_{n-2, 1-\alpha/2} \cdot SE(\widehat{\beta}_0 + \widehat{\beta}_1 X_{\text{new}} + \varepsilon_{\text{new}}),$$ where $SE(\widehat{\beta}_0 + \widehat{\beta}_1 X_{\text{new}} + \varepsilon_{\text{new}}) = \widehat{\sigma} \sqrt{1 + \frac{1}{n} + \frac{(\overline{X} - X_{\text{new}})^2}{\sum_{i=1}^n \left(X_i-\overline{X}\right)^2}}.$

## Sums of squares

 $$\begin{aligned}
   SSE &= \sum_{i=1}^n(Y_i - \widehat{Y}_i)^2 = \sum_{i=1}^n (Y_i - \widehat{\beta}_0 - \widehat{\beta}_1 X_i)^2 \\
   SSR &= \sum_{i=1}^n(\overline{Y} - \widehat{Y}_i)^2 = \sum_{i=1}^n (\overline{Y} - \widehat{\beta}_0 - \widehat{\beta}_1 X_i)^2 \\
   SST &= \sum_{i=1}^n(Y_i - \overline{Y})^2 = SSE + SSR \\
   R^2 &= \frac{SSR}{SST} = 1 - \frac{SSE}{SST} = \widehat{Cor}(\pmb{X},\pmb{Y})^2.
   \end{aligned}$$

## $F$-test in simple linear regression

- *Full (bigger) model :*
   $FM: \qquad Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$
- *Reduced (smaller) model:*
   $RM: \qquad Y_i = \beta_0  + \varepsilon_i$
- The $F$-statistic has the form $F=\frac{(SSE(RM) - SSE(FM)) / (df_{RM} - df_{FM})}{SSE(FM) / df_{FM}}.$
- Reject $H_0: RM$ is correct, if $$F > F_{1-\alpha, 1, n-2}$$ or $$P-value = P\left(F_{1, n-2} > F \right) \leq \alpha.$$

## Assumptions in the simple linear regression model

- $Y_i = \beta_0 + \beta_1 X_{i} + \varepsilon_i$
    - Errors $\varepsilon_i$ are assumed independent $N(0,\sigma^2)$.
    
# Diagnostics for simple linear regression
    
## Diagnostic plots for linearity


```{R}
simple.lm = lm(y2 ~ x2, data=anscombe)
plot(anscombe$x2, resid(simple.lm), 
     ylab='Residual', xlab='X',
     pch=23, bg='orange', cex=1.2)
abline(h=0, lwd=2, col='red', lty=2)
```

## Diagnostic plots for linearity
```{R}
par(mfrow=c(2,2))
plot(simple.lm)
```

## Diagnostic plots for linearity (Quadratic model)

```{R}
quadratic.lm = lm(y2 ~ poly(x2, 2), data=anscombe)
Xsort = sort(anscombe$x2)
plot(anscombe$x2, anscombe$y2, pch=23, 
     bg='orange', cex=1.2, ylab='Y', xlab='X')
lines(Xsort, predict(quadratic.lm, list(x2=Xsort)), 
      col='red', lty=2, lwd=2)
```

## Diagnostic plots for linearity
```{R}
par(mfrow=c(2,2))
plot(quadratic.lm)
```


## Simple linear diagnostics
- Outliers
- Nonconstant variance

## Simple linear diagnostics
```{R}
url = 'http://stats191.stanford.edu/data/HIV.VL.table'
viral.load = read.table(url, header=T)
plot(viral.load$GSS, viral.load$VL, pch=23, 
     bg='orange', cex=1.2)
viral.lm = lm(VL ~ GSS, data=viral.load)
abline(viral.lm, col='red', lwd=2)
```

# Multiple linear regression

## Multiple linear regression model
- Rather than one predictor, we have $p=6$ predictors.
- $Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip} + \varepsilon_i$
  - Errors $\varepsilon$ are assumed independent $N(0,\sigma^2)$, as in simple linear regression.
  - Coefficients are called (partial) regression coefficients because they "allow" for the effect of other variables.

## Overall $F$-test

- *Full (bigger) model :*
   $$Y_i = \beta_0 + \beta_1 X_{i1} + \dots \beta_p X_{ip} + \varepsilon_i$$
- *Reduced (smaller) model:*
   $$Y_i = \beta_0  + \varepsilon_i$$
- The $F$-statistic has the form $F=\frac{(SSE(R) - SSE(F)) / (df_R - df_F)}{SSE(F) / df_F}.$

## Matrix formulation

- ${\vY}_{n \times 1} = {\mX}_{n \times (p + 1)} {\vbeta}_{(p+1) \times 1} + {\varepsilon}_{n \times 1}$
- ${\mX}$ is called the *design matrix* of the model
- ${\vep} \sim N(0, \sigma^2 \mI_{n \times n})$ is multivariate normal $SSE$ in matrix form
$$SSE(\beta) = ({\vY} - {\mX} {\vbeta})'({\vY} - {\mX} {\vbeta})$$


## OLS estimators

- Normal equations yield
$$\widehat{\vbeta} = ({\mX}^T{\mX})^{-1}{\mX}^T{\vY}$$
- Properties: $$\hat{\vbeta}  \sim N(\vbeta, \sigma^2 (\mX^T\mX)^{-1} )$$


## Confidence interval for $\sum_{j=0}^p a_j \beta_j$

- Suppose we want a $(1-\alpha)\cdot 100\%$ CI for $\sum_{j=0}^p a_j\beta_j$.
- Just as in simple linear regression:
    - $\sum_{j=0}^p a_j \widehat{\beta}_j \pm t_{1-\alpha/2, n-p-1} \cdot SE\left(\sum_{j=0}^p a_j\widehat{\beta}_j\right).$
    - Standard error:
$$SE\left(\sum_{j=0}^p a_j\widehat{\beta}_j\right) = \sqrt{\hat{\sigma}^2 \va^T(\mX^T\mX)^{-1}\va}$$


## General $F$-tests

- Given two models $R \subset F$ (i.e. $R$ is a subspace of $F$), we can consider testing 
$$H_0:  \text{$R$ is adequate (i.e. $\mathbb{E}(Y) \in R$)}$$ vs. $$H_a: \text{$F$ is adequate (i.e. $\mathbb{E}(Y) \in F$)}$$
    - The test statistic is $$ F = \frac{(SSE(R) - SSE(F)) / (df_R - df_F)}{SSE(F)/df_F} $$
    -   If $H_0$ is true, $F \sim F_{df_R-df_F, df_F}$ so we reject $H_0$ at level $\alpha$ if $F > F_{df_R-df_F, df_F, 1-\alpha}$.

## Diagnostics: What can go wrong?

- Regression function can be wrong: maybe regression function should have some other form (see diagnostics for simple linear regression).

- Model for the errors may be incorrect:
    -   may not be normally distributed.
    -   may not be independent.
    -   may not have the same variance.

- Detecting problems is more *art* then *science*, i.e. we cannot *test* for all possible problems in a regression model.

- Basic idea of diagnostic measures: if model is correct then residuals $e_i = Y_i -\widehat{Y}_i, 1 \leq i \leq n$ should look like a sample of (not quite independent) $N(0, \sigma^2)$ random variables.

# Diagnostics
## Diagnostics

```{R}
url = 'http://stats191.stanford.edu/data/scottish_races.table'
races.table = read.table(url, header=T)
attach(races.table)
races.lm = lm(Time ~ Distance + Climb)
```
  
## Diagnostics
```{r}
par(mfrow=c(2,2))
plot(races.lm, pch=23 ,bg='orange',cex=1.2)
```

## Diagnostics measures

- DFFITS: $$DFFITS_i = \frac{\widehat{Y}_i - \widehat{Y}_{i(i)}}{\widehat{\sigma}_{(i)} \sqrt{H_{ii}}}$$

- Cook's Distance: $$D_i = \frac{\sum_{j=1}^n(\widehat{Y}_j - \widehat{Y}_{j(i)})^2}{(p+1) \, \widehat{\sigma}^2}$$

- DFBETAS: $$DFBETAS_{j(i)} = \frac{\widehat{\beta}_j - \widehat{\beta}_{j(i)}}{\sqrt{\widehat{\sigma}^2_{(i)} (X^TX)^{-1}_{jj}}}.$$

## Diagnostics measures
```{R}
influence.measures(races.lm)
```    

## Outliers

- Observations $(Y, X_1, \dots, X_p)$ that do not follow the model, while most other observations seem to follow the model.
- One solution: Bonferroni correction, threshold at $t_{1 - \alpha/(2*n), n-p-2}$.
- Bonferroni: if we are doing many $t$ (or other) tests, say $m >>1$ we can control overall false positive rate at $\alpha$ by testing each one at level $\alpha/m$.

## Outliers
```{R}
library(car)
outlierTest(races.lm)
```

# Interactions and ANOVA

## Qualitative variables and interactions

```{R}
url = 'http://stats191.stanford.edu/data/jobtest.table'
jobtest.table = read.table(url, header=T)
jobtest.table$MINORITY = factor(jobtest.table$MINORITY)
plot(jobtest.table$TEST, jobtest.table$JPERF, type='n')
points(jobtest.table$TEST[(jobtest.table$MINORITY == 0)], jobtest.table$JPERF[(jobtest.table$MINORITY == 0)], pch=21, cex=1.2, bg='purple')
points(jobtest.table$TEST[(jobtest.table$MINORITY == 1)], jobtest.table$JPERF[(jobtest.table$MINORITY == 1)], pch=25, cex=1.2, bg='green')
```

## Qualitative variables and interactions
```{R}
jobtest.lm1 = lm(JPERF ~ TEST, jobtest.table)
plot(jobtest.table$TEST, jobtest.table$JPERF, type='n')
points(jobtest.table$TEST[(jobtest.table$MINORITY == 0)], jobtest.table$JPERF[(jobtest.table$MINORITY == 0)], pch=21, cex=1.2, bg='purple')
points(jobtest.table$TEST[(jobtest.table$MINORITY == 1)], jobtest.table$JPERF[(jobtest.table$MINORITY == 1)], pch=25, cex=1.2, bg='green')
abline(jobtest.lm1$coef, lwd=3, col='blue')
```

## Qualitative variables and interactions
```{R}
jobtest.lm4 = lm(JPERF ~ TEST * MINORITY, data = jobtest.table)
print(summary(jobtest.lm4))

```

## Qualitative variables and interactions
```{r}
plot(jobtest.table$TEST, jobtest.table$JPERF, type='n')
points(jobtest.table$TEST[(jobtest.table$MINORITY == 0)], jobtest.table$JPERF[(jobtest.table$MINORITY == 0)], pch=21, cex=1.2, bg='purple')
points(jobtest.table$TEST[(jobtest.table$MINORITY == 1)], jobtest.table$JPERF[(jobtest.table$MINORITY == 1)], pch=25, cex=1.2, bg='green')
abline(jobtest.lm4$coef['(Intercept)'], jobtest.lm4$coef['TEST'], lwd=3, col='purple')
abline(jobtest.lm4$coef['(Intercept)'] + jobtest.lm4$coef['MINORITY1'],
      jobtest.lm4$coef['TEST'] + jobtest.lm4$coef['TEST:MINORITY1'], lwd=3, col='green')
```

##  ANOVA models: one-way

\tiny
\begin{tabular}{|l|l|l|l|l|}
\hline
Source &	SS &	df	& MS & $\E \left(\text{MS}\right)$\\
\hline 
Treatment & $\text{SSTR}=\sum_{i=1}^r n_i \left(\overline{Y}_{i\cdot} - \overline{Y}_{\cdot\cdot}\right)^2$ & $r-1$ & $\text{MSTR} = \dfrac{\text{SSTR}}{r-1}$ & $\begin{aligned} \sigma^{2} & +\\ \frac{\sum_{i=1}^r n_i \alpha_i^2}{r-1}& \end{aligned}$\\
Error & $\text{SSE}=\sum_{i=1}^r \sum_{j=1}^{n_i}(Y_{ij} - \overline{Y}_{i\cdot})^2$ & $\sum_{i=1}^r (n_i - 1)$ & $\text{MSE} = \dfrac{\text{SSE}}{\sum_{i=1}^r (n_i - 1)}$ & $\sigma^2$\\
\hline
\end{tabular}

## ANOVA models: two-way
- In the balanced case, everything can again be summarized
from the ANOVA table
\tiny
\begin{tabular}{|l|l|l|l|}
\hline
Source & SS & DF & MS \\
\hline 
A & $SSA=nm\sum_{i=1}^r  \left(\overline{Y}_{i\cdot\cdot} - \overline{Y}_{\cdot\cdot\cdot}\right)^2$ & r-1 & SSA/(r-1) \\
B & $SSB=nr\sum_{j=1}^m  \left(\overline{Y}_{\cdot j\cdot} - \overline{Y}_{\cdot\cdot\cdot}\right)^2$ & m-1 & SSB/(m-1) \\
A:B & $SSAB = n\sum_{i=1}^r \sum_{j=1}^m  \left(\overline{Y}_{ij\cdot} - \overline{Y}_{i\cdot\cdot} - \overline{Y}_{\cdot j\cdot} + \overline{Y}_{\cdot\cdot\cdot}\right)^2$ & (m-1)(r-1) & SSAB/(m-1)(r-1) \\
ERROR & $SSE = \sum_{i=1}^r \sum_{j=1}^m \sum_{k=1}^{n}(Y_{ijk} - \overline{Y}_{ij\cdot})^2$ & (n-1)mr & SSE/(n-1)mr \\
\hline
\end{tabular}

## ANOVA models: two-way

\begin{tabular}{|l|l|}
\hline
Source &  $\mathbb{E}(MS)$\\
\hline 
A &  $\sigma^2 + nm\frac{\sum_{i=1}^r \alpha_i^2}{r-1}$\\
B &  $\sigma^2 + nr\frac{\sum_{j=1}^m \beta_j^2}{m-1}$\\
A:B & $\sigma^2 + n\frac{\sum_{i=1}^r\sum_{j=1}^m (\alpha\beta)_{ij}^2}{(r-1)(m-1)}$\\
ERROR &  $\sigma^2$\\
\hline
\end{tabular}

# Weighted Least Squares

## Weighted Least Squares

- A way to correct for errors with unequal variance (**but we need a model of the variance**).
- Weighted Least Squares $$SSE(\beta, w) = \sum_{i=1}^n w_i \left(Y_i - \beta_0 - \beta_1 X_i\right)^2.$$
- In general, weights should be like: $$w_i = \frac{1}{\text{Var}(\varepsilon_i)}.$$
- WLS estimator:
$$\hat{\beta}_W = (X^TWX)^{-1}(X^TWY).$$

- If weights are ignored standard errors are wrong! 
- Briefly talked about efficiency of estimators.


# Autocorrelation.

## Correlated errors: NASDAQ daily close 2011


```{R}
url = 'http://stats191.stanford.edu/data/nasdaq_2011.csv'
nasdaq.data = read.table(url, header=TRUE, sep=',')

plot(nasdaq.data$Date, nasdaq.data$Close, xlab='Date', ylab='NASDAQ close',
     pch=23, bg='red', cex=1.2)
```

## ACF
```{R}
acf(nasdaq.data$Close)
```

## AR(1) noise

- Suppose that, instead of being independent, the errors in our model were $\varepsilon_t = \rho \cdot \varepsilon_{t-1} + \omega_t, \qquad -1 < \rho < 1$ with $\omega_t \sim N(0,\sigma^2)$ independent.
- If $\rho$ is close to 1, then errors are very correlated, $\rho=0$ is independence.
- This is "Auto-Regressive Order (1)" noise (AR(1)). Many other models of correlation exist: ARMA, ARIMA, ARCH, GARCH, etc.

## Correcting for AR(1) 

- Suppose we know $\rho$, if we "whiten" the data and regressors $$\begin{aligned}
     \tilde{Y}_{t+1} &= Y_{t+1} - \rho Y_t, t > 1   \\
     \tilde{X}_{(t+1)j} &= X_{(t+1)j} - \rho X_{tj}, i > 1
     \end{aligned}$$ for $1 \leq t \leq n-1$. This model satisfies "usual" assumptions, i.e. the errors $\tilde{\varepsilon}_t = \omega_{t+1} = \varepsilon_{t+1} - \rho \cdot \varepsilon_t$ are independent $N(0,\sigma^2)$.
- For coefficients in new model $\tilde{\beta}$, $\beta_0 = \tilde{\beta}_0 / (1 - \rho)$, $\beta_j = \tilde{\beta}_j.$
- Problem: in general, we donâ€™t know $\rho$, but estimated it.
- If correlation structure is ignored standard errors are wrong! 
- Another example of **whitening when we can model the variance.**

# Bootstrap

## Bootstrapping `lm`

- Using WLS (weighted least squares) requires a model for the variance of $\epsilon$ given $X$.
- Ignoring this changing variance (heteroskedasticity) and using OLS leads to bad intervals, p-values, etc. **because standard errors are incorrect.**
- The (pairs) bootstrap uses the OLS estimator but is able to get a **correct estimator of standard error**.

## Bootstrapping `lm`
```{R}
library(car)
n = 50
X = rexp(n)
Y = 3 + 2.5 * X + X * (rexp(n) - 1) # our usual model is false here! W=X^{-2}
Y.lm = lm(Y ~ X)
pairs.Y.lm = Boot(Y.lm, coef, method='case', R=1000)
confint(pairs.Y.lm, type='norm') # using bootstrap SE
```

# Model selection

## Model selection criteria

- Mallow's $C_p$:
$$C_p({\cal M}) = \frac{SSE({\cal M})}{\widehat{\sigma}^2} + 2 \cdot p({\cal M}) - n.$$
- Akaike (AIC) defined as $$AIC({\cal M}) = - 2 \log L({\cal M}) + 2 p({\cal M})$$ where $L({\cal M})$ is the maximized likelihood of the model.
- Bayes (BIC) defined as $$BIC({\cal M}) = - 2 \log L({\cal M}) + \log n \cdot p({\cal M})$$
- Adjusted $R^2$
- Stepwise (`step`) vs. best subsets (`leaps`).

## $K$-fold cross-validation 

- Fix a model ${\cal M}$. 
- Break data set into $K$ approximately equal sized groups $(G_1, \dots, G_K)$.
- for (i in 1:K)
    - Use all groups except $G_i$ to fit model, predict outcome in group $G_i$ based on this model $\widehat{Y}_{j,{\cal M}, G_i}, j \in G_i$.
- Estimate $$CV({\cal M}) = \frac{1}{n}\sum_{i=1}^K \sum_{j \in G_i} (Y_j - \widehat{Y}_{j,{\cal M},-G_i})^2.$$

# Multicollinearity

## Multicollinearity
- Detecting collinearity
    - Large values of pairwise correlation coefficient, the regression results do not conform to prior expectations
    - Variance inflation factors, condition indices
- Working with Collinear data
    - Standardization
    - Principal components regression
    - Penalization


# Penalized regression

## Shrinkage estimator

- In one sample problem, when trying to estimate $\mu$ from $Y_i \sim N(\mu, \sigma^2)$ we looked at the estimator
$$\hat{Y}_{\alpha} = \alpha \cdot \bar{Y}.$$

- The "quality" of the estimator decomposed as
$$E((\hat{Y}_{\alpha}-\mu)^2) = \text{Bias}(\hat{Y}_{\alpha})^2 + \text{Var}(\hat{Y}_{\alpha})$$

## Shrinkage estimator
```{R}
nsample = 40
ntrial = 500
mu = 0.5
sigma = 2.5
MSE = function(mu.hat, mu) {
  return(sum((mu.hat - mu)^2) / length(mu))
}

alpha = seq(0.0,1,length=20)

mse = numeric(length(alpha))
```

## Shrinkage estimator
```{r}
for (i in 1:ntrial) {
  Z = rnorm(nsample) * sigma + mu
  for (j in 1:length(alpha)) {
    mse[j] = mse[j] + MSE(alpha[j] * mean(Z) * rep(1, nsample), 
                          mu * rep(1, nsample)) / ntrial
  }
}
```

## Shrinkage estimator
```{R}
plot(alpha, mse, type='l', lwd=2, col='red', 
     ylim=c(0, max(mse)),
     xlab=expression(paste('Shrinkage parameter,', alpha)), 
     ylab=expression(paste('MSE(', alpha, ')')), 
     cex.lab=1.2)
```


## Ridge regression

$$
\hat{\beta}_{\lambda} = \text{argmin}_{\beta} \frac{1}{2n} \|Y-X\beta\|^2_2 + \lambda \|\beta\|_2^2
$$

<!-- ## Ridge regression -->
<!-- ```{R} -->
<!-- library(lars) -->
<!-- data(diabetes) -->
<!-- library(MASS) -->
<!-- diabetes.ridge = lm.ridge(diabetes$y ~ diabetes$x, -->
<!--                           lambda=exp(seq(0,log(1e3),length=100))) -->
<!-- plot(diabetes.ridge, lwd=3) -->
<!-- ``` -->

<!-- ## Ridge regression -->
<!-- ```{R} -->
<!-- par(cex.lab=1.2) -->
<!-- plot(diabetes.ridge$lambda, diabetes.ridge$GCV, xlab='Lambda', ylab='GCV', type='l', lwd=3, col='orange') -->
<!-- select(diabetes.ridge) -->
<!-- ``` -->

## Ridge with `glmnet`

```{R}
library(lars)
data(diabetes)
plot(glmnet(diabetes$x, diabetes$y, alpha=0))
```

## Ridge with `glmnet`
```{R}
plot(cv.glmnet(diabetes$x, diabetes$y, alpha=0))
```

## LASSO

$$\hat{\beta}_{\lambda} = \text{argmin}_{\beta} \frac{1}{2n} \|Y-X\beta\|^2_2 + \lambda \|\beta\|_1$$

<!-- ## LASSO -->
<!-- ```{R} -->
<!-- diabetes.lasso = lars(diabetes$x, diabetes$y, type='lasso') -->
<!-- plot(diabetes.lasso) -->
<!-- ``` -->

<!-- ## LASSO -->
<!-- ```{R} -->
<!-- par(cex.lab=1.2) -->
<!-- cv.lars(diabetes$x, diabetes$y, K=10, type='lasso') -->
<!-- ``` -->

## LASSO with `glmnet`

```{R}
library(glmnet)
plot(glmnet(diabetes$x, diabetes$y))
```

## LASSO with `glmnet`
```{R}
plot(cv.glmnet(diabetes$x, diabetes$y))
```

# Logistic regression
## Logistic regression model

-  Logistic model $$E(Y|X) = \pi(X) = \frac{\exp(X^T\beta)}{1 + \exp(X^T\beta)}$$
- This automatically fixes $0 \leq E(Y) = \pi(X) \leq 1$.
- The logistic transform:
   $\text{logit}(\pi(X)) = \log\left(\frac{\pi(X)}{1 - \pi(X)}\right) = X^T\beta$
- An example of a *generalized linear model*
    - link function $\text{logit}(\pi(X)) = X^T\beta$
    - Variance function: $\text{Var}(Y|X) = \pi(X)(1 - \pi(X))$

## Odds Ratios

- One reason logistic models are popular is that the parameters have simple interpretations in terms of odds.
- Logistic model: $$OR_{X_j} = \frac{ODDS(Y=1|\dots, X_j=x_j+h, \dots)}{ODDS(Y=1|\dots, X_j=x_j, \dots)} = e^{h\beta_j}$$
- If $X_j \in {0, 1}$ is dichotomous, then odds for group with $X_j = 1$ are $e^{\beta_j}$ higher, other parameters being equal.

## Deviance

- For logistic regression model ${\cal M}$, $DEV({\cal M})$ replaces $SSE({\cal M})/\sigma^2$.
- In least squares regression, we use $$\frac{SSE({\cal M}_R) - SSE({\cal M}_F)}{\sigma^2} \sim  \chi^2_{df_R-df_F}$$
- This is replaced with $DEV({\cal M}_R) - DEV({\cal M}_F) \overset{n \rightarrow \infty}{\sim} \chi^2_{df_R-df_F}$
- For Poisson and binary regression, $\sigma^2=1$ (dispersion parameter of `glm`).

# Poisson regression
## Poisson log-linear regression model

- Log-linear model $$E(Y|X) = \exp(X^T\beta)$$
- This automatically fixes $E(Y|X) \geq 0$.
   
- An example of a *generalized linear model*
    - link function $\text{log}(E(Y|X)) = X^T\beta$
    - Variance function: $\text{Var}(Y|X) = E(Y|X)$

- Interpretation:
$$\frac{E(Y|\dots, X_j=x_j+h, \dots)}{E(Y|\dots, X_j=x_j, \dots)} = e^{h\beta_j}$$


## Reference
- Lecture notes of  [\blc Jonathan Taylor \bc](http://statweb.stanford.edu/~jtaylo/).

