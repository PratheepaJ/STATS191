best.model.Cp = election.leaps$which[indcp,]
best.model.Cp
plot(election.leaps$size,
election.leaps$Cp, pch=23,
bg='orange', cex=2)
n = nrow(X)
p = 7 + 1 # sigma^2 is unknown
AIC_calculated = n * log(2*pi*sum(resid(election.lm)^2)/n) + n + 2*p
c(AIC_calculated, AIC(election.lm))
# k = 2 gives the AIC, k = log(n) refers to BIC
election.step.forward = step(lm(V ~ 1, election.table),
list(upper = ~ I + D + W + G + G:I + P + N),
direction='forward', k=2, trace=FALSE)
election.step.forward
##summary(election.step.forward)
knitr::include_graphics("Lecture_27_election_stepwise_forward.png")
election.step.forward.BIC = step(lm(V ~ 1, election.table),
list(upper = ~ I + D + W +G:I + P + N),
direction='forward', k=log(nrow(X)))
election.step.forward.BIC = step(lm(V ~ 1,
election.table),
list(upper = ~ I + D + W +G:I + P + N),
direction='forward', k=log(nrow(X)))
knitr::include_graphics("Lecture_27_election_forward_stepwise_BIC.png")
#summary(election.step.forward.BIC)
knitr::include_graphics("Lecture_27_foward_stepwise_BIC_summary.png")
election.step.backward = step(election.lm,
direction='backward')
knitr::include_graphics("Lecture_27_backward_stepwise.png")
# summary(election.step.backward)
knitr::include_graphics("Lecture_27_backward_stepwise_summary.png")
library(boot)
#Fitting Generalized Linear Models
election.glm = glm(V ~ ., data=election.table)
# 5-fold cross-validation
# The first component is the raw cross-validation
# estimate of prediction error.
# The second component is the adjusted cross-validation
#  estimate.
#  The adjustment is designed to compensate for
#  the bias introduced by not using
#  leave-one-out cross-validation.
cv.glm(model.frame(election.glm),
election.glm, K=5)$delta
election.leaps = leaps(X, election.table$V,
nbest=3, method='Cp')
V = election.table$V
election.leaps$CV = 0 * election.leaps$Cp
for (i in 1:nrow(election.leaps$which)) {
subset = c(1:ncol(X))[election.leaps$which[i,]]
if (length(subset) > 1) {
Xw = X[,subset]
wlm = glm(V ~ Xw)
election.leaps$CV[i] = cv.glm(model.frame(wlm),
wlm, K=5)$delta[1]
}
else {
Xw = X[,subset[1]]
wlm = glm(V ~ Xw)
election.leaps$CV[i] = cv.glm(model.frame(wlm),
wlm, K=5)$delta[1]
}
}
plot(election.leaps$Cp, election.leaps$CV,
pch=23, bg='orange', cex=2)
plot(election.leaps$size, election.leaps$CV,
pch=23, bg='orange', cex=2)
indcp_5fold = which((election.leaps$CV==
min(election.leaps$CV)))
best.model.Cv = election.leaps$which[indcp_5fold,]
best.model.Cv
X_HIV = read.table('http://stats191.stanford.edu/data/NRTI_X.csv', header=FALSE, sep=',')
Y_HIV = read.table('http://stats191.stanford.edu/data/NRTI_Y.txt', header=FALSE, sep=',')
set.seed(0)
Y_HIV = as.matrix(Y_HIV)[,1]
X_HIV = as.matrix(X_HIV)
nrow(X_HIV)
D = data.frame(X_HIV, Y_HIV)
M = lm(Y_HIV ~ ., data=D)
M_forward = step(lm(Y_HIV ~ 1, data=D), list(upper=M),
trace=FALSE, direction='forward')
#M_forward
knitr::include_graphics("Lecture_27_HIV_forward.png")
M_backward = step(M, list(lower= ~  1),
trace=FALSE, direction='backward')
#M_backward
knitr::include_graphics("Lecture_27_HIV_backward.png")
M_both1 = step(M, list(lower= ~  1, upper=M),
trace=FALSE, direction='both')
#M_both1
knitr::include_graphics("Lecture_27_HIV_both_M1.png")
M_both2 = step(lm(Y_HIV ~ 1, data=D),
list(lower= ~  1, upper=M),
trace=FALSE, direction='both')
#M_both2
knitr::include_graphics("Lecture_27_HIV_both_M2.png")
sort(names(coef(M_forward)))
sort(names(coef(M_backward)))
sort(names(coef(M_both1)))
sort(names(coef(M_both2)))
knitr::include_graphics("Lecture_27_HIV_summary_search_selection.png")
M_backward_BIC = step(M, list(lower= ~  1), trace=FALSE,
direction='backward', k=log(633))
M_forward_BIC = step(lm(Y_HIV ~ 1, data=D), list(upper=M),
trace=FALSE, direction='forward', k=log(633))
M_both1_BIC = step(M, list(upper=M, lower=~1),
trace=FALSE, direction='both', k=log(633))
M_both2_BIC = step(lm(Y_HIV ~ 1, data=D), list(upper=M, lower=~1),
trace=FALSE, direction='both', k=log(633))
install.packages("selectiveInference")
sort(names(coef(M_backward_BIC)))
sort(names(coef(M_forward_BIC)))
sort(names(coef(M_both1_BIC)))
sort(names(coef(M_both2_BIC)))
install.packages("lars")
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 4, message=FALSE, warning=FALSE, cache = TRUE)
set.seed(0)
library(ggplot2)
library(magrittr)
library(dplyr)
library(MASS)
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 4, message=FALSE, warning=FALSE, cache = TRUE)
set.seed(0)
library(ggplot2)
library(magrittr)
library(dplyr)
mu = 0.5
sigma = 5
nsample = 100
ntrial = 1000
MSE = function(mu.hat, mu) {
return(sum((mu.hat - mu)^2) / length(mu))
}
alpha = seq(0, 1,length=20)
mse = numeric(length(alpha))
bias = (1 - alpha) * mu
variance = alpha^2 * 25 / 100
for (i in 1:ntrial) {
Z = rnorm(nsample) * sigma + mu
for (j in 1:length(alpha)) {
mse[j] = mse[j] +
MSE(alpha[j] * mean(Z) * rep(1, nsample),
mu * rep(1, nsample))
}
}
mse = mse / ntrial
plot(alpha, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Shrinkage factor,', alpha)),
ylab=expression(paste('MSE(', alpha, ')')),
cex.lab=1.2)
plot(alpha, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Shrinkage factor,', alpha)),
ylab=expression(paste('MSE(', alpha, ')')),
cex.lab=1.2)
lines(alpha, bias^2, col='green', lwd=2)
lines(alpha, variance, col='blue', lwd=2)
lam = nsample / alpha - nsample
plot(lam, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Penalty parameter,',  lambda)),
ylab=expression(paste('MSE(', lambda, ')')))
plot(lam, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Penalty parameter,',  lambda)),
ylab=expression(paste('MSE(', lambda, ')')))
lines(lam, bias^2, col='green', lwd=2)
lines(lam, variance, col='blue', lwd=2)
plot(alpha, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Shrinkage parameter ', alpha)),
ylab=expression(paste('MSE(', alpha, ')')))
abline(v=mu^2/(mu^2+sigma^2/nsample), col='blue', lty=2)
library(lars)
data(diabetes)
head(diabetes)
dim()
dim(diabetes)
dim(diabetes$x)
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 4, message=FALSE, warning=FALSE, cache = TRUE)
set.seed(0)
library(ggplot2)
library(magrittr)
library(dplyr)
mu = 0.5
sigma = 5
nsample = 100
ntrial = 1000
MSE = function(mu.hat, mu) {
return(sum((mu.hat - mu)^2) / length(mu))
}
alpha = seq(0, 1,length=20)
mse = numeric(length(alpha))
bias = (1 - alpha) * mu
variance = alpha^2 * 25 / 100
for (i in 1:ntrial) {
Z = rnorm(nsample) * sigma + mu
for (j in 1:length(alpha)) {
mse[j] = mse[j] +
MSE(alpha[j] * mean(Z) * rep(1, nsample),
mu * rep(1, nsample))
}
}
mse = mse / ntrial
plot(alpha, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Shrinkage factor,', alpha)),
ylab=expression(paste('MSE(', alpha, ')')),
cex.lab=1.2)
plot(alpha, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Shrinkage factor,', alpha)),
ylab=expression(paste('MSE(', alpha, ')')),
cex.lab=1.2)
lines(alpha, bias^2, col='green', lwd=2)
lines(alpha, variance, col='blue', lwd=2)
lam = nsample / alpha - nsample
plot(lam, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Penalty parameter,',  lambda)),
ylab=expression(paste('MSE(', lambda, ')')))
plot(lam, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Penalty parameter,',  lambda)),
ylab=expression(paste('MSE(', lambda, ')')))
lines(lam, bias^2, col='green', lwd=2)
lines(lam, variance, col='blue', lwd=2)
plot(alpha, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Shrinkage parameter ', alpha)),
ylab=expression(paste('MSE(', alpha, ')')))
abline(v=mu^2/(mu^2+sigma^2/nsample), col='blue', lty=2)
library(lars)
data(diabetes)
library(MASS)
diabetes.ridge = lm.ridge(diabetes$y ~ diabetes$x,
lambda=seq(0, 100, 0.5))
plot(diabetes.ridge, lwd=3)
CV = function(Z, alpha, K=5) {
cve = numeric(K)
n = length(Z)
for (i in 1:K) {
g = seq(as.integer((i-1)*n/K)+1,
as.integer((i*n/K)))
mu.hat = mean(Z[-g]) * alpha
cve[i] = sum((Z[g]-mu.hat)^2)
}
return(c(sum(cve)/n, sd(cve)/sqrt(n)))
}
alpha = seq(0.0,1,length=20)
mse = numeric(length(alpha))
avg.cv = numeric(length(alpha))
for (i in 1:ntrial) {
Z = rnorm(nsample) * sigma + mu
for (j in 1:length(alpha)) {
current_cv = CV(Z, alpha[j])
avg.cv[j] = avg.cv[j] + current_cv[1]
}
}
avg.cv = avg.cv/ntrial
plot(alpha, avg.cv, type='l', lwd=2, col='green',
xlab='Shrinkage parameter, alpha',
ylab='Average CV(alpha)')
abline(v=mu^2/(mu^2+sigma^2/nsample),
col='blue', lty=2)
abline(v=min(alpha[avg.cv == min(avg.cv)]),
col='red', lty=2)
cv = numeric(length(alpha))
cv.sd = numeric(length(alpha))
nsample = 1000
Z = rnorm(nsample) * sigma + mu
for (j in 1:length(alpha)) {
current_cv = CV(Z, alpha[j])
cv[j] = current_cv[1]
cv.sd[j] = current_cv[2]
}
plot(alpha, cv, type='l', lwd=2, col='green',
xlab='Shrinkage parameter, alpha',
ylab='CV(alpha)', xlim=c(0,1))
abline(v=mu^2/(mu^2+sigma^2/nsample),
col='blue', lty=2)
abline(v=min(alpha[cv == min(cv)]),
col='red', lty=2)
par(cex.lab=1.5)
plot(diabetes.ridge$lambda, diabetes.ridge$GCV,
xlab='Lambda', ylab='GCV', type='l',
lwd=3, col='orange')
select(diabetes.ridge)
install.packages("ElemStatLearn")
# Probit model
## Probit model
* $Y$ : number of customers visting store from region;
* $X_1$ : number of housing units in region;
* $X_2$ : average household income;
* $X_3$ : average housing unit age in region;
* $X_4$ : distance to nearest competitor;
* $X_5$ : distance to store in miles.
## Reference
- **CH** Chapter
- Lecture notes of  [\blc Jonathan Taylor \bc](http://statweb.stanford.edu/~jtaylo/).
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 4, message=FALSE, warning=FALSE, cache = TRUE)
set.seed(0)
library(ggplot2)
library(magrittr)
library(dplyr)
mu = 0.5
sigma = 5
nsample = 100
ntrial = 1000
MSE = function(mu.hat, mu) {
return(sum((mu.hat - mu)^2) / length(mu))
}
alpha = seq(0, 1,length=20)
mse = numeric(length(alpha))
bias = (1 - alpha) * mu
variance = alpha^2 * 25 / 100
for (i in 1:ntrial) {
Z = rnorm(nsample) * sigma + mu
for (j in 1:length(alpha)) {
mse[j] = mse[j] +
MSE(alpha[j] * mean(Z) * rep(1, nsample),
mu * rep(1, nsample))
}
}
mse = mse / ntrial
plot(alpha, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Shrinkage factor,', alpha)),
ylab=expression(paste('MSE(', alpha, ')')),
cex.lab=1.2)
plot(alpha, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Shrinkage factor,', alpha)),
ylab=expression(paste('MSE(', alpha, ')')),
cex.lab=1.2)
lines(alpha, bias^2, col='green', lwd=2)
lines(alpha, variance, col='blue', lwd=2)
lam = nsample / alpha - nsample
plot(lam, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Penalty parameter,',  lambda)),
ylab=expression(paste('MSE(', lambda, ')')))
plot(lam, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Penalty parameter,',  lambda)),
ylab=expression(paste('MSE(', lambda, ')')))
lines(lam, bias^2, col='green', lwd=2)
lines(lam, variance, col='blue', lwd=2)
plot(alpha, mse, type='l', lwd=2, col='red',
ylim=c(0, max(mse)),
xlab=expression(paste('Shrinkage parameter ', alpha)),
ylab=expression(paste('MSE(', alpha, ')')))
abline(v=mu^2/(mu^2+sigma^2/nsample), col='blue', lty=2)
library(lars)
data(diabetes)
library(MASS)
diabetes.ridge = lm.ridge(diabetes$y ~ diabetes$x,
lambda=seq(0, 100, 0.5))
plot(diabetes.ridge, lwd=3)
CV = function(Z, alpha, K=5) {
cve = numeric(K)
n = length(Z)
for (i in 1:K) {
g = seq(as.integer((i-1)*n/K)+1,
as.integer((i*n/K)))
mu.hat = mean(Z[-g]) * alpha
cve[i] = sum((Z[g]-mu.hat)^2)
}
return(c(sum(cve)/n, sd(cve)/sqrt(n)))
}
alpha = seq(0.0,1,length=20)
mse = numeric(length(alpha))
avg.cv = numeric(length(alpha))
for (i in 1:ntrial) {
Z = rnorm(nsample) * sigma + mu
for (j in 1:length(alpha)) {
current_cv = CV(Z, alpha[j])
avg.cv[j] = avg.cv[j] + current_cv[1]
}
}
avg.cv = avg.cv/ntrial
plot(alpha, avg.cv, type='l', lwd=2, col='green',
xlab='Shrinkage parameter, alpha',
ylab='Average CV(alpha)')
abline(v=mu^2/(mu^2+sigma^2/nsample),
col='blue', lty=2)
abline(v=min(alpha[avg.cv == min(avg.cv)]),
col='red', lty=2)
cv = numeric(length(alpha))
cv.sd = numeric(length(alpha))
nsample = 1000
Z = rnorm(nsample) * sigma + mu
for (j in 1:length(alpha)) {
current_cv = CV(Z, alpha[j])
cv[j] = current_cv[1]
cv.sd[j] = current_cv[2]
}
plot(alpha, cv, type='l', lwd=2, col='green',
xlab='Shrinkage parameter, alpha',
ylab='CV(alpha)', xlim=c(0,1))
abline(v=mu^2/(mu^2+sigma^2/nsample),
col='blue', lty=2)
abline(v=min(alpha[cv == min(cv)]),
col='red', lty=2)
par(cex.lab=1.5)
plot(diabetes.ridge$lambda, diabetes.ridge$GCV,
xlab='Lambda', ylab='GCV', type='l',
lwd=3, col='orange')
select(diabetes.ridge)
knitr::include_graphics("Lecture_28_lassofig.png")
library(lars)
data(diabetes)
diabetes.lasso = lars(diabetes$x, diabetes$y,
type='lasso')
10/12
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 4, message=FALSE, warning=FALSE, cache = TRUE)
set.seed(0)
library(ggplot2)
library(magrittr)
library(dplyr)
flu.table = read.table('http://stats191.stanford.edu/data/flu.table',
header=TRUE)
flu.glm = glm(Shot ~ Age + Health.Aware,
data=flu.table,
family=binomial())
head(flu.table)
dim(flu.table)
summary(flu.glm)
exp(-1.429284+3.647052)
-1.429284/0.22178
plot(flu.glm, which =1)
head(fitted(flu.glm))
log(0.006582150/(1-0.006582150))
influence.measures(flu.glm)
step(flu.glm, scope=list(upper= ~.^2),
direction='both')
library(ElemStatLearn)
data(spam)
library(ElemStatLearn)
data(spam)
dim(spam)
X = model.matrix(spam ~ ., data=spam)[,-1]
Y = as.numeric(spam$spam == 'spam')
G = glmnet(X, Y, family='binomial')
library(glmnet)
G = glmnet(X, Y, family='binomial')
plot(G)
CV = cv.glmnet(X, Y, family='binomial')
plot(CV)
c(CV$lambda.min, CV$lambda.1se)
beta.hat = coef(G, s=CV$lambda.1se)
beta.hat
summary(glm(Shot ~ Age + Health.Aware,
data=flu.table,
family=binomial(link='probit')))
summary(glm(Shot ~ Age + Health.Aware,
data=flu.table,
family=binomial(link='probit')))
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 4, message=FALSE, warning=FALSE, cache = TRUE)
set.seed(0)
library(ggplot2)
library(magrittr)
library(dplyr)
library(lars)
data(diabetes)
head(diabetes$x)
diabetes$y
diabetes$x
View(diabetes$x)
dim(diabetes$x)
dim(diabetes$x)
library(lars)
data(diabetes)
diabetes.lasso = lars(diabetes$x, diabetes$y,
type='lasso')
diabetes.lasso
summary(diabetes.lasso)
plot(diabetes.lasso, xvar = "norm")
X = matrix(diabetes$x)
View(X)
View(diabetes$x %>% data.frame())
diabetes$x
summary(diabetes.lasso)
diabetes.lasso
X_HIV = read.table('http://stats191.stanford.edu/data/NRTI_X.csv',
header=FALSE, sep=',')
Y_HIV = read.table('http://stats191.stanford.edu/data/NRTI_Y.txt',
header=FALSE, sep=',')
set.seed(0)
Y_HIV = as.matrix(Y_HIV)[,1]
X_HIV = as.matrix(X_HIV)
dim9X_HIV
dim(X_HIV)
library(glmnet)
G = glmnet(X_HIV, Y_HIV)
plot(G)
CV = cv.glmnet(X_HIV, Y_HIV)
plot(CV)
beta.hat = coef(G, s=CV$lambda.1se)
beta.hat # might want to use as.numeric(beta.hat) instead of a sparse vector
class(beta.hat)
sum(beta.hat[,1] >0)
CV = cv.glmnet(X_HIV, Y_HIV, alpha=0.25)
Enet = glmnet(X_HIV, Y_HIV, alpha=0.25)
plot(Enet)
beta.hat = coef(Enet, s=CV$lambda.1se)
beta.hat
sum(abs(beta.hat[,1]) > 0)
sum(abs(beta.hat[,1]) >0)
(8*38+10*32+6*24)/24
qt(.025, 17, lower.tail = T)
