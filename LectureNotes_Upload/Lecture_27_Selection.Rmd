---
title: "Lecture 27:  Selection"
shorttitle: "STATS 191 Lecture 27"
author: "Pratheepa Jeganathan"
date: "11/22/2019"
output: 
  beamer_presentation:
    colortheme: "seahorse"
    slide_level: 2
    includes:
      in_header: header.tex
bibliography: AppliedStat.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 4, message=FALSE, warning=FALSE, cache = TRUE)
set.seed(0)
library(ggplot2)
library(magrittr)
library(dplyr)
```


## Recap

- What is a regression model?
- Descriptive statistics -- graphical
- Descriptive statistics -- numerical
- Inference about a population mean
- Difference between two population means
- Some tips on R
- Simple linear regression (covariance, correlation, estimation, geometry of least squares)
    - Inference on simple linear regression model
    - Goodness of fit of regression: analysis of variance.
    - $F$-statistics.
    - Residuals.
    - Diagnostic plots for simple linear regression (graphical methods).

## Recap
- Multiple linear regression
    - Specifying the model.
    - Fitting the model: least squares.
    - Interpretation of the coefficients.
    - Matrix formulation of multiple linear regression
    - Inference for multiple linear regression
        - $T$-statistics revisited.
        - More $F$ statistics.
        - Tests involving more than one $\beta$.   
- Diagnostics – more on graphical methods and numerical methods
    - Different types of residuals
    - Influence
    - Outlier detection
    - Multiple comparison (Bonferroni correction)
    - Residual plots:
        - partial regression (added variable) plot,
        - partial residual (residual plus component) plot.

## Recap
- Adding qualitative predictors 
    - Qualitative variables as predictors to the regression model.
    - Adding interactions to the linear regression model. 
    - Testing for equality of regression relationship in various subsets of a population
- ANOVA
    - All qualitative predictors.
    - One-way layout
    - Two-way layout
- Transformation
    - Achieving linearity
    - Stabilize variance
    - Weighted least squares
- Correlated Errors
    - Generalized least squares
- Bootstrapping linear regression


# Selection
## Outline (Model selection)

- In a given regression situation, there are often many choices to be made. 
- Recall our usual setup
$$
Y_{n \times 1} = X_{n \times p} \beta_{p \times 1} + \epsilon_{n \times 1}.
$$

- Any *subset $A \subset \{1, \dots, p\}$* yields a new regression model
$$
{\cal M}(A): Y_{n \times 1} = X[,A] \beta[A] + \epsilon_{n \times 1}
$$
by setting $\beta[A^c]=0$.

- **Model selection** is, roughly speaking, how to choose $A$ among the $2^p$ possible choices.

## Election data

Here is a dataset from the book that we will use to explore different model selection approaches.

\begin{tabular}{|l|l|}
\hline
Variable & Description \\
\hline
$V$ & votes for a presidential candidate\\
$I$ & are they incumbent?\\
$D$ & Democrat or Republican incumbent?\\
$W$ & wartime election?\\
$G$ & GDP growth rate in election year\\
$P$ & (absolute) GDP deflator growth rate\\
$N$ & number of quarters in which GDP growth rate $> 3.2\%$\\
\hline
\end{tabular}

## Election data
```{r fig.show='hide'}
url = 'http://stats191.stanford.edu/data/election.table'
election.table = read.table(url, header=T)
pairs(election.table[,2:ncol(election.table)], 
      cex.labels=3, pch=23,
      bg='orange', cex=2)
```

## Election data
```{r echo=FALSE, fig.width=12, fig.height=12}
pairs(election.table[,2:ncol(election.table)], 
      cex.labels=3, pch=23,
      bg='orange', cex=2)
```

## Problem & Goals

- When we have many predictors (with many possible interactions), it can be difficult to find a good model.
- Which main effects do we include?
- Which interactions do we include?
- Model selection procedures try to *simplify / automate* this task.
- Election data has $2^6=64$ different models with just main effects!

## General comments

- This is generally an "unsolved" problem in statistics: there are no magic procedures to get you the "best model."

- Many machine learning methods look for good "sparse" models: selecting a "sparse" model.

- "Machine learning" often work with very many predictors.

- Our model selection problem is generally at a much smaller scale than "data mining" problems.

- Still, it is a hard problem.

<!-- - **Inference after selection is full of pitfalls!**  -->

## Hypothetical example
- Suppose we fit a a model $F: \quad Y_{n \times 1} = X_{n \times p} \beta_{p \times 1} + \varepsilon_{n \times 1}$ with predictors ${ X}_1, \dots, { X}_p$.
- In reality, some of the $\beta$’s may be zero. Let’s suppose that $\beta_{j+1}= \dots= \beta_{p}=0$.
- Then, any model that includes $\beta_0, \dots, \beta_j$ is *correct*: which model gives the *best* estimates of $\beta_0, \dots, \beta_j$?
- Principle of *parsimony* (i.e. Occam’s razor) says that the model with *only* ${X}_1, \dots, {X}_j$ is "best".

## Justifying parsimony

- For simplicity, let’s assume that $j=1$ so there is only one coefficient to estimate.
- Then, because each model gives an *unbiased* estimate of $\beta_1$ we can compare models based on $\text{Var}(\widehat{\beta}_1).$
- The best model, in terms of this variance, is the one containing only ${ X}_1$.
- What if we didn’t know that only $\beta_1$ was non-zero (which we don’t know in general)?
- In this situation, we must choose a set of variables.

## Model selection: choosing a subset of variables

- To "implement" a model selection procedure, we first need a criterion or benchmark to compare two models.
- Given a criterion, we also need a search strategy.
- With a limited number of predictors, it is possible to search all possible models (`leaps` in `R`).

# Candidate criteria

## Candidate criteria

Possible criteria:

- $R^2$: not a good criterion. Always increase with model size $\implies$ "optimum" is to take the biggest model.
- Adjusted $R^2$: better. It "penalized" bigger models. Follows principle of parsimony / Occam’s razor.
- Mallow’s $C_p$ – attempts to estimate a model’s predictive power, i.e. the power to predict a new observation.

## Best subsets, $R^2$

- Leaps takes a design matrix as argument: throw away the intercept column or leaps will complain.

```{r}
election.lm = lm(V ~ I + D + W + G:I + 
                   P + N, election.table)
#election.lm
```

```{r echo=FALSE,out.width = "250px"}
knitr::include_graphics("Lecture_27_election_lm_summary.png")
```

## Best subsets, $R^2$
```{r}
X = model.matrix(election.lm)[,-1]
library(leaps)
# Since the algorithm returns a best model of each size, 
# the results do not depend on a penalty model for 
# model size
# nbest: Number of subsets of each size to report
election.leaps = leaps(x = X, y = election.table$V, 
  nbest=3, method='r2')
```

- Find out the predictors in the model with the largest $R^{2}$:
```{r}
# election.leaps$which: matrix, each row can be 
# used to select the columns of x in the respective model
ind = which((election.leaps$r2 == max(election.leaps$r2)))
best.model.r2 = election.leaps$which[ind, ]
best.model.r2
```


## Best subsets, $R^2$
- Let's plot the $R^2$ as a function of the model size. 

```{r fig.show='hide'}
plot(election.leaps$size, election.leaps$r2, 
  pch=23, bg='orange', cex=2, 
  xlab = "Size of the model", 
  ylab = bquote(R^2))
```


## Best subsets, $R^2$
- For example, there are three models with 2 predictors and with different $R^{2}$
- We see that the full model does include all variables and has the largest $R^{2}$. 
```{r echo=FALSE}
plot(election.leaps$size, election.leaps$r2, 
  pch=23, bg='orange', cex=2, 
  xlab = "Size of the model", 
  ylab = bquote(R^2))
```


## Best subsets, adjusted $R^2$

- As we add more and more variables to the model – even random ones, $R^2$ will increase to 1.

- Adjusted $R^2$ tries to take this into account by replacing sums of squares by *mean squares* $$R^2_a = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)} = 1 - \frac{MSE}{MST}.$$

## Best subsets, adjusted $R^2$
```{r}
election.leaps = leaps(X, election.table$V, nbest=3,
  method='adjr2')
ind2 = which((election.leaps$adjr2 ==
    max(election.leaps$adjr2)))
best.model.adjr2 = election.leaps$which[ind2,]
best.model.adjr2
```
- Best model based on the adjusted $R^2$ has four predictor variables.

## Best subsets, adjusted $R^2$
```{R}
plot(election.leaps$size, 
  election.leaps$adjr2, 
     pch=23, bg='orange', cex=2)
```

## Mallow’s $C_p$

- Mallow’s $C_p$ $$C_p({\cal M}) = \frac{SSE({\cal M})}{\widehat{\sigma}^2} + 2 \cdot p({\cal M}) - n.$$
    - $\widehat{\sigma}^2=SSE(F)/df_F$ is the "best" estimate of $\sigma^2$ we have (use the fullest model), i.e. in the election data it uses all 6 main effects.
    - $SSE({\cal M})$ is the $SSE$ of the model ${\cal M}$.
    - $p({\cal M})$ is the number of predictors in ${\cal M}$.
- This is an estimate of the expected mean-squared error of $\widehat{Y}({\cal M})$, it takes *bias* and *variance* of fit into account.
- Account for the sample size, effect size of the predictors, and  collinearity between the predictors.

## Best subsets, Mallow’s $C_p$
```{r}
election.leaps = leaps(X, election.table$V, nbest=3,
  method='Cp')
indcp = which((election.leaps$Cp == 
    min(election.leaps$Cp)))
best.model.Cp = election.leaps$which[indcp,]
best.model.Cp
```

## Best subsets, Mallow’s $C_p$
```{r}
plot(election.leaps$size, 
  election.leaps$Cp, pch=23, 
  bg='orange', cex=2)
```

# Search strategies 

## Search strategies 

- Given a criterion, we now have to decide how we are going to search through the possible models.

- "Best subset": search all possible models and take the one with highest $R^2_a$ or lowest $C_p$ leaps. Such searches are typically feasible only up to $p=30$ or $40$ at the very most.

- Stepwise (forward, backward or both): useful when the number of predictors is large. Choose an initial model and be "greedy".
    - "Greedy" means always take the biggest jump (up or down) in your selected criterion.


## Implementations in `R`

- "Best subset": use the function `leaps`. Works only for multiple linear regression models.
* Stepwise: use the function `step`. Works for any model with Akaike Information Criterion (AIC). In multiple linear regression, AIC is (almost) a linear function of $C_p$.

## Akaike / Bayes Information Criterion

- Akaike (AIC) defined as $$AIC({\cal M}) = - 2 \log L({\cal M}) + 2 \cdot p({\cal M})$$ where $L({\cal M})$ is the maximized likelihood of the model.
- Bayes (BIC) defined as $$BIC({\cal M}) = - 2 \log L({\cal M}) + \log n \cdot p({\cal M})$$
- Strategy can be used for whenever we have a likelihood, so this generalizes to many statistical models.


## AIC for regression

- In linear regression with unknown $\sigma^2$ $$-2 \log L({\cal M}) = n \log(2\pi \widehat{\sigma}^2_{MLE}) + n$$ where $\widehat{\sigma}^2_{MLE} = \frac{1}{n} SSE(\widehat{\beta})$
- In linear regression with known $\sigma^2$ $$-2 \log L({\cal M}) = n \log(2\pi \sigma^2) + \frac{1}{\sigma^2} SSE(\widehat{\beta})$$ so AIC is very much like Mallow’s $C_p$ in this case.

## AIC for regression
- For the election data, the linear regression with all predictors has
```{r}
n = nrow(X)
p = 7 + 1 # sigma^2 is unknown 
AIC_calculated = n * log(2*pi*sum(resid(election.lm)^2)/n) + n + 2*p
c(AIC_calculated, AIC(election.lm))
```

## Properties of AIC / BIC

- BIC will typically choose a model as small or smaller than AIC (if using the same search direction).
- As our sample size grows, under some assumptions, it can be shown that
     - AIC will (asymptotically) always choose a model that contains the true model, i.e. it won’t leave any variables out.
     - BIC will (asymptotically) choose exactly the right model.
     
## Election example

- Let's take a look at `step` in action. 
- Probably the simplest strategy is *forward stepwise* which tries to add one variable at a time, as long as it can find a resulting model whose AIC is better than its current position. 
- When it can make no further additions, it terminates.

## Election example (forward stepwise)
```{r}
# k = 2 gives the AIC, k = log(n) refers to BIC
election.step.forward = step(lm(V ~ 1, election.table),
  list(upper = ~ I + D + W + G + G:I + P + N),
  direction='forward', k=2, trace=FALSE)
election.step.forward
```

##
- Summary of the chosen model based on forward stepwise and AIC.
```{r}
##summary(election.step.forward)
```

```{r echo=FALSE,out.width = "250px"}
knitr::include_graphics("Lecture_27_election_stepwise_forward.png")
```

## Interactions and hierarchy

- We notice that although the *full* model we gave it had the interaction `I:G`, the function `step` never tried to use it. 
- This is due to some rules implemented in `step` that do not include an interaction unless both main effects are already in the model. 
- In this case, because neither $I$ nor $G$ were added, the interaction was never considered.
- In the `leaps` example, we gave the function the design matrix
and it did not have to consider interactions: they were already encoded in the design matrix.

## BIC example

- The only difference between AIC and BIC is the price paid
per variable. This is the argument `k` to `step`. 
- By default `k=2` and for BIC we set `k=log(n)`. 
- If we set `k=0` it will always add variables.

```{r results='hide', include=FALSE}
election.step.forward.BIC = step(lm(V ~ 1, election.table),
  list(upper = ~ I + D + W +G:I + P + N),
  direction='forward', k=log(nrow(X)))
```

```{r message=FALSE}
election.step.forward.BIC = step(lm(V ~ 1,
election.table),
  list(upper = ~ I + D + W +G:I + P + N),
  direction='forward', k=log(nrow(X)))
```

## BIC example
```{r echo=FALSE,out.width = "200px"}
knitr::include_graphics("Lecture_27_election_forward_stepwise_BIC.png")
```

## BIC example
```{r}
#summary(election.step.forward.BIC)
```

```{r echo=FALSE,out.width = "250px"}
knitr::include_graphics("Lecture_27_foward_stepwise_BIC_summary.png")
```

## Backward selection

- Let's consider backwards stepwise. This starts at a full
model and tries to delete variables.
- There is also a `direction="both"` option.

```{r message=FALSE}
election.step.backward = step(election.lm, 
  direction='backward')
```

## Backward selection
```{r echo=FALSE,out.width = "100px"}
knitr::include_graphics("Lecture_27_backward_stepwise.png")
```

## Backward selection
```{r}
# summary(election.step.backward)
```

```{r echo=FALSE,out.width = "200px"}
knitr::include_graphics("Lecture_27_backward_stepwise_summary.png")
```

## Cross-validation

- Yet another model selection criterion is $K$-fold cross-validation.
- Fix a model ${\cal M}$. Break data set into $K$ approximately equal sized groups $(G_1, \dots, G_K)$.
- For (i in 1:K) Use all groups except $G_i$ to fit model, predict outcome in group $G_i$ based on this model $\widehat{Y}_{j,{\cal M}, G_i}, j \in G_i$.
- Similar to what we saw in Cook's distance / DFFITS.
- Estimate $CV({\cal M}) = \frac{1}{n}\sum_{i=1}^K \sum_{j \in G_i} (Y_j - \widehat{Y}_{j,{\cal M},G_i})^2.$

## Comments about cross-validation.

- It is a general principle that can be used in other situations to "choose parameters."
- Pros (partial list): "objective" measure of a model's predictive power.
- Cons (partial list): all we know about inference is *usually* "out the window" (also true for other model selection procedures).
- If goal is not really inference about certain specific parameters, it is a reasonable way to compare models.

## Example (Cross-validation)
```{r}
library(boot)
#Fitting Generalized Linear Models
election.glm = glm(V ~ ., data=election.table)
# 5-fold cross-validation
# The first component is the raw cross-validation 
# estimate of prediction error. 
# The second component is the adjusted cross-validation
#  estimate. 
#  The adjustment is designed to compensate for 
#  the bias introduced by not using 
#  leave-one-out cross-validation.
cv.glm(model.frame(election.glm), 
  election.glm, K=5)$delta
```

## $C_p$ versus 5-fold cross-validation

- Let's plot our $C_p$ versus the $CV$ score.

- Keep in mind that there is additional randomness in the $CV$ score due to the random assignments to groups. 


## $C_p$ versus 5-fold cross-validation

```{r}
election.leaps = leaps(X, election.table$V, 
                       nbest=3, method='Cp')
V = election.table$V
election.leaps$CV = 0 * election.leaps$Cp
for (i in 1:nrow(election.leaps$which)) {
    subset = c(1:ncol(X))[election.leaps$which[i,]]
    if (length(subset) > 1) {
       Xw = X[,subset]
       wlm = glm(V ~ Xw)
       election.leaps$CV[i] = cv.glm(model.frame(wlm), 
         wlm, K=5)$delta[1]
    }
    else {
       Xw = X[,subset[1]]
       wlm = glm(V ~ Xw)
       election.leaps$CV[i] = cv.glm(model.frame(wlm), 
         wlm, K=5)$delta[1]
    }
}
```

## $C_p$ versus 5-fold cross-validation

```{r}
plot(election.leaps$Cp, election.leaps$CV, 
  pch=23, bg='orange', cex=2)
```

## $C_p$ versus 5-fold cross-validation

```{r}
plot(election.leaps$size, election.leaps$CV, 
  pch=23, bg='orange', cex=2)

```

## $C_p$ versus 5-fold cross-validation
```{r}
indcp_5fold = which((election.leaps$CV==
                       min(election.leaps$CV)))
best.model.Cv = election.leaps$which[indcp_5fold,]
best.model.Cv
```

## Summarizing results

- The model selected depends on the criterion used.

\begin{tabular}{|l|l|}
\hline 
Criterion &  Model\\
\hline
$R^2$ &  $\sim I + D + W +G:I + P + N$ \\
$R^2_a$ &  $\sim I + D + P + N$\\
$C_p$ &  $\sim D+P+N$ \\
AIC forward  & $\sim D+P$ \\
BIC forward &  $\sim D$ \\
AIC backward & $\sim I + D + N + I:G$\\
5-fold CV & ~ $\sim I+W$\\
\hline
\end{tabular}

- **The selected model is random and depends on which method we use!**

## Where we are so far

- Many other "criteria" have been proposed.
- Some work well for some types of data, others for different data.
- Check diagnostics!
- These criteria (except cross-validation) are not "direct measures" of predictive power, though Mallow’s $C_p$ is a step in this direction.
- $C_p$ measures the quality of a model based on both *bias* and *variance* of the model. Why is this important?
- *Bias-variance* tradeoff is ubiquitous in statistics. More soon.

## A larger example

- Resistance of $n=633$ different HIV+ viruses to drug 3TC.
- Features $p=91$ are mutations in a part of the HIV virus, response is log fold change in vitro.

## Example (HIV and mutations)
```{R}
X_HIV = read.table('http://stats191.stanford.edu/data/NRTI_X.csv', header=FALSE, sep=',')
Y_HIV = read.table('http://stats191.stanford.edu/data/NRTI_Y.txt', header=FALSE, sep=',')
set.seed(0)
Y_HIV = as.matrix(Y_HIV)[,1]
X_HIV = as.matrix(X_HIV)
nrow(X_HIV)
```

## Forward stepwise

```{r}
D = data.frame(X_HIV, Y_HIV)
M = lm(Y_HIV ~ ., data=D)
M_forward = step(lm(Y_HIV ~ 1, data=D), list(upper=M),
  trace=FALSE, direction='forward')
#M_forward
```

```{r echo=FALSE,out.width = "200px"}
knitr::include_graphics("Lecture_27_HIV_forward.png")
```

## Backward stepwise

```{r}
M_backward = step(M, list(lower= ~  1), 
  trace=FALSE, direction='backward')
#M_backward
```


```{r echo=FALSE,out.width = "200px"}
knitr::include_graphics("Lecture_27_HIV_backward.png")
```

## Both directions

```{R}
M_both1 = step(M, list(lower= ~  1, upper=M), 
  trace=FALSE, direction='both')
#M_both1
```

```{r echo=FALSE,out.width = "200px"}
knitr::include_graphics("Lecture_27_HIV_both_M1.png")
```

## Both directions
```{r}
M_both2 = step(lm(Y_HIV ~ 1, data=D), 
  list(lower= ~  1, upper=M), 
  trace=FALSE, direction='both')
#M_both2
```

```{r echo=FALSE,out.width = "200px"}
knitr::include_graphics("Lecture_27_HIV_both_M2.png")
```


## Compare selected models

```{r results='hide'}
sort(names(coef(M_forward)))
sort(names(coef(M_backward)))
sort(names(coef(M_both1)))
sort(names(coef(M_both2)))
```

```{r echo=FALSE,out.width = "150px"}
knitr::include_graphics("Lecture_27_HIV_summary_search_selection.png")
```


## BIC vs AIC

```{r}
M_backward_BIC = step(M, list(lower= ~  1), trace=FALSE,
  direction='backward', k=log(633))
M_forward_BIC = step(lm(Y_HIV ~ 1, data=D), list(upper=M),
  trace=FALSE, direction='forward', k=log(633))
M_both1_BIC = step(M, list(upper=M, lower=~1), 
  trace=FALSE, direction='both', k=log(633))
M_both2_BIC = step(lm(Y_HIV ~ 1, data=D), list(upper=M, lower=~1), 
  trace=FALSE, direction='both', k=log(633))
```

## BIC vs AIC
```{r results='hide'}
sort(names(coef(M_backward_BIC)))
sort(names(coef(M_forward_BIC)))
sort(names(coef(M_both1_BIC)))
sort(names(coef(M_both2_BIC)))
```

```{r echo=FALSE,out.width = "150px"}
knitr::include_graphics("Lecture_27_HIV_summary_search_selection_BIC.png")
```

# Inference after selection

## Inference after selection: data snooping and splitting

- Each of the above criteria return a model. The `summary` provides $p$-values.

```{r}
summary(election.step.forward)
```

## Inference after selection 

- We can also form confidence intervals. **But, can we trust these intervals or tests? No!**
- Recommended reading [\blc Work by Jonathan Taylor\bc](https://github.com/jonathan-taylor/selective-inference)
```{r}
library(selectiveInference)
```


## Reference
- **CH** Chapter 11 (Variable selection procedures)
- Lecture notes of  [\blc Jonathan Taylor \bc](http://statweb.stanford.edu/~jtaylo/).